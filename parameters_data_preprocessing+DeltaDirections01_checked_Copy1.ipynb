{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6529c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.plotting import plot_trajectory\n",
    "from utils.processing import (\n",
    "    normalize_angles_2pi,\n",
    "    open_file,\n",
    "    cut_jumps\n",
    "    )\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "#from scipy.ndimage import gaussian_filter1d\n",
    "# from utils.processing import (\n",
    "#     normalize_angles_2pi,\n",
    "#     open_file,\n",
    "#     cut_jumps,\n",
    "#     save_preprocessed_data,\n",
    "#     open_preprocessed_data,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa704884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_evrth(NEW_DT, wall_perc, angInterval, bins):\n",
    "\n",
    "    DATA_DIR = 'dataset'\n",
    "    PREPROC_DATA_DIR = 'downsamp_preprocessed_dataset'\n",
    "\n",
    "    # 1. open files\n",
    "    dataset_names = sorted(glob(os.path.join(DATA_DIR, '*.txt')))\n",
    "    datasets = {}\n",
    "    for nam in dataset_names:\n",
    "        k = Path(nam).stem\n",
    "        v = open_file(nam, show=False)\n",
    "        datasets[k] = v\n",
    "\n",
    "    # 2. cut_jumps\n",
    "    tr11 = datasets['Trace_1.1']  # TODO: params to config\n",
    "    tr12 = datasets['Trace_1.2']  \n",
    "    tr2 = datasets['Trace_2']\n",
    "    tr5 = datasets['Trace_5']\n",
    "\n",
    "    tr4 = cut_jumps(datasets['Trace_4'], xmax=40.4, xmin=-38.1, ymax=28.3, ymin=-47.6,show=False)\n",
    "    tr3 = cut_jumps(datasets['Trace_3'], xmax=46.8, xmin=-48, ymax=46, ymin=-47,show=False)\n",
    "    tr6 = cut_jumps(datasets['Trace_6'], xmax=45, xmin=-45, ymax=45, ymin=-42.2,show=False)\n",
    "    tr7 = cut_jumps(datasets['Trace_7'], xmax=35, xmin=-38.7, ymax=40, ymin=-32,show=False)\n",
    "    tr91 = cut_jumps(datasets['Trace_9.1'], xmax=42.5, xmin=-43.7, ymax=42.5, ymin=-42.7,show=False)\n",
    "    tr92 = cut_jumps(datasets['Trace_9.2'], xmax=36.3, xmin=-46.5, ymax=45.4, ymin=-47.3,show=False)\n",
    "\n",
    "    dfs = [tr11, tr12, tr2, tr3, tr4, tr5, tr6, tr7, tr91, tr92]\n",
    "    dfs_names = ['tr11', 'tr12', 'tr2','tr3','tr4','tr5','tr6','tr7', 'tr91','tr92']\n",
    "\n",
    "    dfs_dict = {}\n",
    "    for k, val in zip(dfs_names, dfs):\n",
    "        dfs_dict[k] = val\n",
    "\n",
    "    # 3. rescale x and y coords to fit area size \n",
    "    # может добавить функцию rescale_coord в processing.py? TODO\n",
    "    def rescale_coord(data, new_min, new_max):\n",
    "        min_old_x, max_old_x = np.min(data), np.max(data)\n",
    "        data_rescaled = ((data - min_old_x) / (max_old_x - min_old_x)) * (new_max - new_min) + new_min\n",
    "        return data_rescaled\n",
    "\n",
    "    AREA_SIZE = [-40, 40]\n",
    "    rescaled_dfs_dict = {}\n",
    "    for k, df in dfs_dict.items():\n",
    "        df = df.dropna()\n",
    "        rescaled_x = rescale_coord(df['x'].to_numpy(), AREA_SIZE[0], AREA_SIZE[1])\n",
    "        rescaled_y = rescale_coord(df['y'].to_numpy(), AREA_SIZE[0], AREA_SIZE[1])\n",
    "        df.loc[:, 'x'] = rescaled_x\n",
    "        df.loc[:, 'y'] = rescaled_y\n",
    "\n",
    "        rescaled_dfs_dict[k] = df\n",
    "\n",
    "    # 4. divide trajectories into sub-trajectories to remove inconsistent dt and Nans\n",
    "    def divide_traj_by_nans(df, k, dt_threshold, traj_drop_threshold=5):\n",
    "        # 1. Drop Nans\n",
    "        df = df.dropna()\n",
    "        # 2. Find if any time jumps\n",
    "        dt_times = df['time'].diff().to_numpy()\n",
    "        dt_times[0] = 0.\n",
    "        div_idx = np.where(dt_times > dt_threshold)[0]\n",
    "        div_idx = np.asarray(div_idx)\n",
    "\n",
    "        # Divide by time jumps\n",
    "        if len(div_idx) > 0:\n",
    "            div_idx = np.concatenate([np.array([0]),\n",
    "                                      div_idx,\n",
    "                                      np.array([len(df)])])\n",
    "            sub_trajectories = []\n",
    "            start_i = div_idx[0]\n",
    "            for end_i in div_idx[1:]:\n",
    "                if end_i - start_i > traj_drop_threshold:\n",
    "                    sub_df = df[start_i:end_i]\n",
    "                    sub_trajectories.append(sub_df)\n",
    "                start_i = end_i + 1\n",
    "\n",
    "            return k, sub_trajectories\n",
    "        else:\n",
    "            return k, [df]\n",
    "\n",
    "    DROP_THRESHOLD = 5\n",
    "    dt_thresholds = {'tr11': 0.035, 'tr12': 0.035, 'tr2': 0.035, 'tr3': 0.135,  # TODO: to configs\n",
    "                     'tr4': 0.035,  'tr5': 0.035, 'tr6': 0.035, 'tr7': 0.035,\n",
    "                     'tr91': 0.035, 'tr92': 0.035}\n",
    "\n",
    "    subtraj_dfs_dict = {}\n",
    "    for k, df in rescaled_dfs_dict.items():\n",
    "        k, df_subtraj= divide_traj_by_nans(df, k, dt_threshold=dt_thresholds[k],\n",
    "                                           traj_drop_threshold=DROP_THRESHOLD)\n",
    "        subtraj_dfs_dict[k] = df_subtraj  # list of dfs\n",
    "\n",
    "    # 5. resample to required dt\n",
    "\n",
    "\n",
    "    def resample_data_to_larger_timestep(df, original_dt, new_dt):\n",
    "        from scipy.signal import resample\n",
    "\n",
    "        num_original_points = df.shape[0]\n",
    "        duration = num_original_points * original_dt\n",
    "        num_new_points = int(duration / new_dt)\n",
    "\n",
    "        resampled_dfnp = resample(df.to_numpy()[:, 1:], num_new_points, axis=0)\n",
    "        new_time = np.linspace(df['time'].iloc[0], df['time'].iloc[-1], num_new_points)\n",
    "        #added 3 down change\n",
    "        #print(resampled_dfnp[:, 0])\n",
    "        #print(resampled_dfnp)\n",
    "        if len(resampled_dfnp) > 1:\n",
    "            resampled_df = pd.DataFrame({'time': new_time,\n",
    "                                         'x': resampled_dfnp[:, 0], \n",
    "                                         'y': resampled_dfnp[:, 1]})\n",
    "        #added 2 down change\n",
    "        else:\n",
    "            resampled_df = pd.DataFrame(columns=['time','x','y'])\n",
    "        return resampled_df\n",
    "\n",
    "    # NOTE: We ignore that there are TWO most frequent dt-s: 0.033 and 0.034\n",
    "    ORIGINAL_DT = {'tr11': 0.034, 'tr12': 0.034, 'tr2': 0.034, 'tr3': 0.134,  # TODO: to configs\n",
    "                   'tr4': 0.034, 'tr5': 0.034, \n",
    "                   'tr6': 0.034, 'tr7': 0.034, 'tr91': 0.034, 'tr92': 0.034}\n",
    "\n",
    "\n",
    "    resampled_subtraj_dfs_dict = {}\n",
    "    for k, df in subtraj_dfs_dict.items():\n",
    "        subtraj_list = []\n",
    "        for sub_traj_df in df:\n",
    "            resampled_sub_traj_df = resample_data_to_larger_timestep(sub_traj_df,\n",
    "                                                                        ORIGINAL_DT[k],\n",
    "                                                                        NEW_DT)\n",
    "            #changed\n",
    "            if len(resampled_sub_traj_df) > 1:\n",
    "                subtraj_list.append(resampled_sub_traj_df)\n",
    "        resampled_subtraj_dfs_dict[k] = subtraj_list\n",
    "\n",
    "\n",
    "    # 6. add angles, delta_angles, step len, \n",
    "    def add_angles(df):\n",
    "        dt = df['time'].diff()\n",
    "        vvx = df['x'].diff() / dt \n",
    "        vvy = df['y'].diff() / dt \n",
    "        dt[0], vvx[0], vvy[0] = 0., 0., 0.\n",
    "\n",
    "        Xx = np.arctan2(vvx, vvy)\n",
    "        Nres = normalize_angles_2pi(Xx)\n",
    "        ang = np.rad2deg(Nres) % 360\n",
    "        df['angles'] = ang\n",
    "        return df\n",
    "\n",
    "    def add_delta_angles(df):\n",
    "        dangles = df['angles'].diff()\n",
    "        dangles[0] = 0.\n",
    "        dangles_rad = np.deg2rad(dangles)\n",
    "        normed_dangels_rad = normalize_angles_2pi(dangles_rad)\n",
    "        df['delta_angle'] = np.rad2deg(normed_dangels_rad) % 360\n",
    "        return df\n",
    "\n",
    "    def add_step_length(df):\n",
    "        dx = df['x'].diff()\n",
    "        dy = df['y'].diff()\n",
    "        dx[0], dy[0] = 0., 0.,\n",
    "        step_lens = (dx**2 + dy**2)**0.5\n",
    "        df['step_length'] = step_lens\n",
    "        return df\n",
    "\n",
    "\n",
    "    dfs_dict_with_angles = {}\n",
    "    for k, df in resampled_subtraj_dfs_dict.items():\n",
    "        subtraj_list = []\n",
    "        for sub_traj_df in df:\n",
    "            df_ang = add_angles(sub_traj_df)\n",
    "            df_dt_ang = add_delta_angles(df_ang)\n",
    "            df_step = add_step_length(df_dt_ang)\n",
    "            subtraj_list.append(df_step)\n",
    "        dfs_dict_with_angles[k] = subtraj_list\n",
    "\n",
    "    # 7. sort be areas of open field\n",
    "    #долго выполняется если рисовать все графики\n",
    "\n",
    "\n",
    "    def sort_walls_area(tr, border_tr, wall_percent=0.1): \n",
    "        ylen = border_tr['ymax'] - border_tr['ymin'] \n",
    "        xlen = border_tr['xmax'] - border_tr['xmin'] \n",
    "        xl = border_tr['xmin'] + xlen * wall_percent\n",
    "        xr = border_tr['xmax'] - xlen * wall_percent\n",
    "        yd = border_tr['ymin'] + ylen * wall_percent\n",
    "        yu = border_tr['ymax'] - ylen * wall_percent\n",
    "\n",
    "        tr['near_wall'] = 0.\n",
    "        for i in range(len(tr)):     \n",
    "            if tr['x'].iloc[i] < xl:\n",
    "                if tr['y'].iloc[i] > yu:\n",
    "                    tr['near_wall'].iloc[i] = 40\n",
    "                elif tr['y'].iloc[i] < yd:\n",
    "                    tr['near_wall'].iloc[i] = 30\n",
    "                else:\n",
    "                    tr['near_wall'].iloc[i] = 4\n",
    "            elif tr['x'].iloc[i] > xr:\n",
    "                if tr['y'].iloc[i] > yu:\n",
    "                    tr['near_wall'].iloc[i] = 10\n",
    "                elif tr['y'].iloc[i] < yd:\n",
    "                    tr['near_wall'].iloc[i] = 20\n",
    "                else:\n",
    "                    tr['near_wall'].iloc[i] = 2\n",
    "            elif tr['y'].iloc[i] > yu:\n",
    "                tr['near_wall'].iloc[i] = 1\n",
    "            elif tr['y'].iloc[i] < yd:\n",
    "                tr['near_wall'].iloc[i] = 3\n",
    "        return tr\n",
    "\n",
    "    def plot_wall_sorted_samples(df, plot_title):\n",
    "        all_areas = [40, 30, 20, 10, 4, 3, 2, 1, 0]\n",
    "        for a in all_areas:\n",
    "            x = df.loc[df['near_wall'] == a]['x']\n",
    "            y = df.loc[df['near_wall'] == a]['y']\n",
    "            plt.scatter(x, y, s=1.5, label=str(a))\n",
    "        plt.gca().set_aspect('equal')\n",
    "        plt.title(plot_title)\n",
    "        plt.legend(loc='center')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    dfs_with_walls = {}\n",
    "    bord40 =  {'xmin': -40,'xmax': 40,'ymin': -40,'ymax': 40}   \n",
    "\n",
    "    for k, df in dfs_dict_with_angles.items():\n",
    "        subtraj_list = []\n",
    "        for sub_df in df:\n",
    "            sub_df = sort_walls_area(sub_df, bord40, wall_perc)\n",
    "            #plot_wall_sorted_samples(sub_df, k)\n",
    "            subtraj_list.append(sub_df)\n",
    "        dfs_with_walls[k] = subtraj_list\n",
    "\n",
    "    # 8. merge dfs of each mouse\n",
    "    #dict, where subtrajectories of 1 mice are merged in 1 df (with time jumps)\n",
    "    #the first row from each df is deleted because it has angle 0 and delta angle 0\n",
    "    merg_resampled_dfs_dict = {}\n",
    "    for k, df in dfs_with_walls.items():\n",
    "        df = [df0.iloc[1:].reset_index() for df0 in df]\n",
    "        merg_resampled_dfs_dict[k] = pd.concat(df)\n",
    "        merg_resampled_dfs_dict[k] = merg_resampled_dfs_dict[k].reset_index(drop=True)\n",
    "    for k,df in merg_resampled_dfs_dict.items():\n",
    "        print(len(df))\n",
    "\n",
    "    # 9.make 4 general dfs (for all mice) for angles +- parallel to the wall in areas 1 and 3, other angles \n",
    "    # in areas 1 and 3, +- parallel to the wall in areas 2 and 4, other angles in areas 2 and 4\n",
    "\n",
    "    #make scatter plot of near wall delta angles with respect to angles\n",
    "\n",
    "    #проверено, насколько уменьшилось количество точек по сравнению с вариантом где не вырезаны первые строки суб траектории\n",
    "    #там 5704, тут 5547\n",
    "    DfsStep13, DfsStep24 = {}, {}\n",
    "    n=0\n",
    "    for k, df in merg_resampled_dfs_dict.items():\n",
    "        DfsStep13[k] = df.loc[(df['near_wall']==1) | (df['near_wall']==3)]\n",
    "        DfsStep13[k] = DfsStep13[k].reset_index()\n",
    "        DfsStep13[k] = DfsStep13[k].drop(['index'], axis=1) \n",
    "        n += len(DfsStep13[k])\n",
    "\n",
    "        DfsStep24[k] = df.loc[(df['near_wall']==2) | (df['near_wall']==4)]\n",
    "        DfsStep24[k] = DfsStep24[k].reset_index()\n",
    "        DfsStep24[k] = DfsStep24[k].drop(['index'], axis=1)\n",
    "\n",
    "\n",
    "    #we took approximately equal number of points from every mouse for par13 (dfs where mice are \n",
    "    #in areas 1 + 3 and their vectors are +- parallelto the wall or not parallel (regular angles)  \n",
    "    par13 = []\n",
    "    reg13 = []\n",
    "    p13len = []\n",
    "    r13len = []\n",
    "    for k, df in DfsStep13.items():\n",
    "        mask = ((df['angles']>=90-angInterval) & (df['angles']<=90+angInterval)) \\\n",
    "                                   | ((df['angles']>=270-angInterval) & (df['angles']<=270+angInterval))\n",
    "        parallel_df = df.loc[mask]\n",
    "        not_parallel_df = df[~mask]\n",
    "        p13len.append(len(parallel_df))\n",
    "        r13len.append(len(not_parallel_df))\n",
    "        par13.append(parallel_df) \n",
    "        reg13.append(not_parallel_df)\n",
    "    print(sorted(p13len), sorted(r13len))\n",
    "    lp13 =round(np.percentile(p13len, 40))\n",
    "    lr13 = round(np.percentile(r13len, 40))\n",
    "    print(lp13,lr13)\n",
    "\n",
    "    for i in range(len(par13)):\n",
    "        if len(par13[i]) > lp13:\n",
    "            par13[i] = par13[i].iloc[0:lp13]\n",
    "    for i in range(len(reg13)):\n",
    "        if len(reg13[i]) > lr13:\n",
    "            reg13[i] = reg13[i].iloc[0:lr13]\n",
    "\n",
    "\n",
    "    dfPar13 = pd.concat(par13)\n",
    "    dfReg13 = pd.concat(reg13)\n",
    "\n",
    "#     plt.scatter(dfReg13['angles'], dfReg13['delta_angle'], s=1, color='bisque', label='not parall')\n",
    "#     plt.scatter(dfPar13['angles'], dfPar13['delta_angle'], s=1, color='darkred', label='parall')\n",
    "#     plt.axvline(90, c='black')\n",
    "#     plt.axvline(270, c='black')\n",
    "#     plt.legend()\n",
    "#     plt.title('How mouse change its angel wrt angle she is currently in')\n",
    "#     plt.xlabel('angle')\n",
    "#     plt.ylabel('delta angle')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "    #in areas 2 + 4  \n",
    "    par24 = []\n",
    "    reg24 = []\n",
    "    p24len = []\n",
    "    r24len = []\n",
    "    for k, df in DfsStep24.items():\n",
    "        mask = ((df['angles']>=90-angInterval) & (df['angles']<=90+angInterval)) \\\n",
    "                                   | ((df['angles']>=270-angInterval) & (df['angles']<=270+angInterval))\n",
    "        mask = ((df['angles']>=360-angInterval) | (df['angles']<=angInterval)) \\\n",
    "                                | ((df['angles']>=180-angInterval) & (df['angles']<=180+angInterval))\n",
    "        parallel_df = df.loc[mask]\n",
    "        not_parallel_df = df[~mask]\n",
    "        p24len.append(len(parallel_df))\n",
    "        r24len.append(len(not_parallel_df))\n",
    "        par24.append(parallel_df) \n",
    "        reg24.append(not_parallel_df)\n",
    "    print(sorted(p24len), sorted(r24len))\n",
    "    lp24 =round(np.percentile(p24len, 40))\n",
    "    lr24 = round(np.percentile(r24len, 40))\n",
    "    print(lp24,lr24)\n",
    "\n",
    "    for i in range(len(par24)):\n",
    "        if len(par24[i]) > lp24:\n",
    "            par24[i] = par24[i].iloc[0:lp24]\n",
    "    for i in range(len(reg24)):\n",
    "        if len(reg24[i]) > lr24:\n",
    "            reg24[i] = reg24[i].iloc[0:lr24]\n",
    "\n",
    "\n",
    "    dfPar24 = pd.concat(par24)\n",
    "    dfReg24 = pd.concat(reg24)\n",
    "\n",
    "#     plt.scatter(dfReg24['angles'], dfReg24['delta_angle'], s=1, color='aquamarine', label='not parall')\n",
    "#     plt.scatter(dfPar24['angles'], dfPar24['delta_angle'], s=1, color='indigo', label='parall')\n",
    "#     plt.axvline(90, c='black')\n",
    "#     plt.axvline(270, c='black')\n",
    "#     plt.legend()\n",
    "#     plt.title('How mouse change its angel wrt angle she is currently in')\n",
    "#     plt.xlabel('angle')\n",
    "#     plt.ylabel('delta angle')\n",
    "#     plt.show()\n",
    "\n",
    "    # 10. make hists of near wall delta angle distributions near the wall. we consider 2 types of angles: angles that are\n",
    "    #  +- parallel to the walls and others  \n",
    "    wall_hist = {}\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 3))\n",
    "    names = ['par13', 'reg13', 'par24', 'reg24']\n",
    "    for i, dat in enumerate([dfPar13, dfReg13, dfPar24, dfReg24]):\n",
    "        freqs, binEdges, _ = axes[i].hist(dat['delta_angle'], bins=bins)\n",
    "        if i ==0:\n",
    "            print(f'Bin size is {binEdges[1] - binEdges[0]} (degrees)')\n",
    "        for ang in [0, 90, 180, 360]:\n",
    "            axes[i].axvline(ang, c='black', alpha=0.5)\n",
    "        axes[i].set_title(f'delta angles for {names[i]}')\n",
    "\n",
    "        bin_mid = 0.5 * (binEdges[1:] + binEdges[:-1])\n",
    "        wall_hist[names[i]] = [bin_mid, freqs]\n",
    "    plt.show()\n",
    "\n",
    "    #all together\n",
    "#     fig, axes = plt.subplots(1, 4, figsize=(20, 4))\n",
    "#     u = 0\n",
    "#     for i, h in wall_hist.items():\n",
    "#         #axes[u]  = fig.add_subplot(2, 2, 1)\n",
    "#         axes[u].plot(h[0], h[1])\n",
    "#         axes[u].set_title(f'Delta angle distribution, {i} area, bins={bins}')\n",
    "#         u += 1\n",
    "#     plt.show()\n",
    "\n",
    "    #all normalized\n",
    "    def min_max_normalize(y):\n",
    "        yn = (y - y.min()) / (y.max() - y.min())\n",
    "        return yn\n",
    "\n",
    "    for k, df in wall_hist.items():\n",
    "        df[1] = min_max_normalize(df[1])\n",
    "        plt.plot(df[0], df[1], label=k)\n",
    "    plt.title(f'dAng distributions in near wall area, bins={bins}, dt={NEW_DT}, perc={wall_perc}, +-ang={angInterval}')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"pict_parameters_deltaanddistr/nearwall_distr_bins_\"+str(bins)+'_dt_'+str(NEW_DT)+'_perc_'+str(wall_perc)+'_+-ang_'+str(angInterval)+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # all together\n",
    "    # for i, dat in enumerate([dfPar13, dfReg13, dfPar24, dfReg24]):\n",
    "    #     plt.hist(dat['delta_angle'], label=names[i], alpha=0.5, bins=bins)\n",
    "    # for ang in [0, 90, 180, 360]:\n",
    "    #     plt.axvline(ang, c='black', alpha=0.5)\n",
    "    # plt.title('All delta angles together')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "    # 10. we took the same amount of point from every mouse from every type of area (center(area 0), near walls, corners)\n",
    "    # to build averaged angle/delta angle distributions\n",
    "\n",
    "    MaxLenDict = {}\n",
    "    all_l = []\n",
    "    for area in [1, 2, 3, 4]:\n",
    "        for k, df in merg_resampled_dfs_dict.items():    \n",
    "            all_l.append(len(df.loc[df[\"near_wall\"] == area]))\n",
    "    MaxLenDict['nearWalls'] = round(np.percentile(all_l, 40))\n",
    "\n",
    "    all_l=[]\n",
    "    for k, df in merg_resampled_dfs_dict.items():    \n",
    "        all_l.append(len(df.loc[df[\"near_wall\"] == 0]))\n",
    "    MaxLenDict['0'] = round(np.percentile(all_l, 40))\n",
    "\n",
    "    all_l=[]\n",
    "    for area in [10, 20, 30, 40]:\n",
    "        for k, df in merg_resampled_dfs_dict.items():    \n",
    "            all_l.append(len(df.loc[df[\"near_wall\"] == area]))\n",
    "    MaxLenDict['corner'] = round(np.percentile(all_l, 40))\n",
    "\n",
    "    print(MaxLenDict)\n",
    "\n",
    "    def cut_into_dict(dfs_dict, area, max_len):\n",
    "        cutted_df_dict = {}\n",
    "        for k, df in dfs_dict.items():\n",
    "            # print(k + ' ' + str(area))  # tmp\n",
    "            cutted_df = df.loc[df['near_wall'] == area]\n",
    "            # print(len(cutted_df))  # tmp\n",
    "            cutted_df = cutted_df.iloc[:max_len]\n",
    "            # print(len(cutted_df))  # tmp\n",
    "            cutted_df_dict[k] = cutted_df\n",
    "\n",
    "        return cutted_df_dict\n",
    "\n",
    "    walls = [10, 20, 30, 40]\n",
    "    near_corner_dict = {}\n",
    "    for w in walls:\n",
    "        near_corner_dict[w] = cut_into_dict(merg_resampled_dfs_dict, w, MaxLenDict['corner'])\n",
    "\n",
    "    walls = [1, 2, 3, 4]  # TODO different max_len ???\n",
    "    near_wall_dict = {}\n",
    "    for w in walls:\n",
    "        near_wall_dict[w] = cut_into_dict(merg_resampled_dfs_dict, w, MaxLenDict['nearWalls'])\n",
    "\n",
    "    walls = [0]\n",
    "    near_center_dict = {}\n",
    "    for w in walls:\n",
    "        near_center_dict[w] = cut_into_dict(merg_resampled_dfs_dict, w, MaxLenDict['0'])\n",
    "\n",
    "    def merge_df(df_dict):\n",
    "        delta_ang_list = []\n",
    "        for k, dfdfdf in df_dict.items():\n",
    "            for l, df in dfdfdf.items():\n",
    "                delta_ang_list.append(df['delta_angle'])\n",
    "            dfN = pd.concat(delta_ang_list)\n",
    "            dfN = dfN.reset_index()\n",
    "            dfN = dfN.drop(['index'], axis=1)\n",
    "        return dfN\n",
    "\n",
    "    #хз плохо ли что для распределений в разные типы зон берется разное количество точек. можно взять одинаковое\n",
    "    #понятно что из полученных словарей (near_wall_dict и тд) легко построить распределение для каждой зоны\n",
    "\n",
    "    #made df for every zone (corners, center, walls)\n",
    "    aver_delta_angles = {}\n",
    "\n",
    "    aver_corner = merge_df(near_corner_dict)\n",
    "    print(f'For area corner df len={len(aver_corner)}')\n",
    "    aver_delta_angles['corner'] = aver_corner\n",
    "\n",
    "    aver_wall = merge_df(near_wall_dict)\n",
    "    print(f'For area wall df len={len(aver_wall)}')\n",
    "    aver_delta_angles['wall'] = aver_wall\n",
    "\n",
    "    aver_center = merge_df(near_center_dict)\n",
    "    print(f'For area center df len={len(aver_center)}')\n",
    "    aver_delta_angles['center'] = aver_center\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    delta_ang_hist = {}\n",
    "\n",
    "    freqs, binEdges, _ = axes[0].hist(aver_delta_angles['corner'], bins=bins)\n",
    "    bin_mid = 0.5 * (binEdges[1:] + binEdges[:-1])\n",
    "    delta_ang_hist['corner']  = [bin_mid, freqs]\n",
    "    axes[0].plot(bin_mid, freqs)\n",
    "    axes[0].set_title(f'All mice, all corners [10, 20, 30, 40], bins={bins}')\n",
    "\n",
    "    freqs, binEdges, _ = axes[1].hist(aver_delta_angles['wall'], bins=bins)\n",
    "    bin_mid = 0.5 * (binEdges[1:] + binEdges[:-1])\n",
    "    delta_ang_hist['wall']  = [bin_mid, freqs]\n",
    "    axes[1].plot(bin_mid, freqs)\n",
    "    axes[1].set_title(f'All mice, all walls [1, 2, 3, 4], bins={bins}')\n",
    "\n",
    "    freqs, binEdges, _ = axes[2].hist(aver_delta_angles['center'], bins=bins)\n",
    "    bin_mid = 0.5 * (binEdges[1:] + binEdges[:-1])\n",
    "    delta_ang_hist['center']  = [bin_mid, freqs]\n",
    "    axes[2].plot(bin_mid, freqs)\n",
    "    axes[2].set_title(f'All mice, center [0], bins={bins}')\n",
    "\n",
    "    plt.figure()\n",
    "    for k, l in delta_ang_hist.items():\n",
    "        l[1] = min_max_normalize(l[1])\n",
    "        plt.plot(l[0], l[1], label=k)\n",
    "    plt.title(f'dAng distributions in all areas, bins={bins}, dt={NEW_DT}, perc={wall_perc}, +-ang={angInterval}')\n",
    "    plt.legend()\n",
    "    plt.savefig(\"pict_parameters_deltaanddistr/all_areas_distr_bins_\"+str(bins)+'_dt_'+str(NEW_DT)+'_perc_'+str(wall_perc)+'_+-ang_'+str(angInterval)+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "    # # Save distributions\n",
    "    # import pickle\n",
    "    # with open(f'dec23_angWallsDistr_nbins_{bins}.pkl', 'wb') as fp:\n",
    "    #     pickle.dump(delta_ang_hist, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "for NEW_DT in [0.1,0.3,0.5,0.7,1,2,3,5]:\n",
    "    for wall_perc in [0.05, 0.1, 0.15, 0.2]:\n",
    "        for angInterval in [10,25,32,40]:\n",
    "            for bins in [20,40,60,80,100]:\n",
    "                make_evrth(NEW_DT,wall_perc,angInterval,bins)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6507d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wall_perc 0.05 instead of 0.5\n",
    "for NEW_DT in [0.1,0.3,0.5,0.7,1,2,3,5]:\n",
    "    for wall_perc in [0.05]:\n",
    "        for angInterval in [10,25,32,40]:\n",
    "            for bins in [20,40,60,80,100]:\n",
    "                make_evrth(NEW_DT,wall_perc,angInterval,bins)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7429092",
   "metadata": {},
   "source": [
    "#parameter to change! time lag between neighboring points \n",
    "\n",
    "NEW_DT = 0.5\n",
    "\n",
    "#parameter to change! how much of the length of the square field side we consider as the area close to the wall \n",
    "\n",
    "wall_perc=0.1\n",
    "\n",
    "#parameter to change! If angle is in 90+-angInterval (as well as 270+-angInterval) we consider it as an angle \n",
    "#approximately parallel to the front wall\n",
    "\n",
    "angInterval = 25\n",
    "\n",
    "#parameter to change! number of bins for building averaged angle distributions\n",
    "\n",
    "bins = 80\n",
    "\n",
    "\n",
    "make_evrth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46c224",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8633c321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b44e686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d6c0508",
   "metadata": {},
   "source": [
    "### draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22702ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save distributions\n",
    "# import pickle\n",
    "# with open(f'angWallsDistr_nbins_{bins}.pkl', 'wb') as fp:\n",
    "#     pickle.dump(wall_hist, fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18be4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#норм ли будет усреднять углы с разным количествои точек?\n",
    "MaxLenDict = {}\n",
    "all_l = []\n",
    "for area in [1, 2, 3, 4]:\n",
    "    for k, df in merg_resampled_dfs_dict.items():    \n",
    "        all_l.append(len(df.loc[df[\"near_wall\"] == area]))\n",
    "MaxLenDict['nearWalls'] = round(np.percentile(all_l, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6fcc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In 9 all areas\n",
    "fig, axes = plt.subplots(2, 5, figsize=(16, 6))\n",
    "j, i = 0, 0\n",
    "for k in merg_resampled_dfs_dict:\n",
    "    merg_resampled_dfs_dict[k].plot.scatter(x='angles',y='delta_angle', c='DarkBlue',s=1, ax=axes[j, i])\n",
    "    axes[j, i].set_title(k)\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        j = 1\n",
    "        i = 0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5245828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# m = 0\n",
    "# for k, df in resampled_subtraj_dfs_dict.items():\n",
    "#     n = 0\n",
    "#     for sub_traj_df in df:\n",
    "#         n += len(sub_traj_df)\n",
    "#     m += n\n",
    "#     print(k,n)\n",
    "# print('all together', m)\n",
    "# #delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# m = 0\n",
    "# for k, df in resampled_subtraj_dfs_dict.items():\n",
    "#     n = 0\n",
    "#     for sub_traj_df in df:\n",
    "#         n += len(sub_traj_df)\n",
    "#     m += n\n",
    "#     print(k,n)\n",
    "# print('all together', m)\n",
    "\n",
    "# #delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# #dict, where subtrajectories of 1 mice are merged in 1 df (with time jumps)\n",
    "# merg_resampled_dfs_dict = {}\n",
    "# for k, df in resampled_subtraj_dfs_dict.items():\n",
    "#     merg_resampled_dfs_dict[k] = pd.concat(df)\n",
    "#     merg_resampled_dfs_dict[k] = merg_resampled_dfs_dict[k].reset_index(drop=True)\n",
    "# print(merg_resampled_dfs_dict)\n",
    "# #delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# #dict, where subtrajectories of 1 mice are merged in 1 df (with time jumps)\n",
    "# merg_resampled_dfs_dict = {}\n",
    "# for k, df in resampled_subtraj_dfs_dict.items():\n",
    "#     merg_resampled_dfs_dict[k] = pd.concat(df)\n",
    "#     merg_resampled_dfs_dict[k] = merg_resampled_dfs_dict[k].reset_index(drop=True)\n",
    "# print(merg_resampled_dfs_dict)\n",
    "\n",
    "#delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# for k, df in merg_resampled_dfs_dict.items():\n",
    "#     plot_wall_sorted_samples(df,k)\n",
    "# #delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# for k, df in merg_resampled_dfs_dict.items():\n",
    "#     plot_wall_sorted_samples(df,k)\n",
    "#delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# for k, df in merg_resampled_dfs_dict.items():\n",
    "#     print(k, 'all', len(df))\n",
    "#     print(k,'1+3', len(df.loc[(df['near_wall']==1) | (df['near_wall']==3)]))\n",
    "#     print(k,'ratio', (len(df.loc[(df['near_wall']==1) | (df['near_wall']==3)]) / len(df)))\n",
    "# #delete TODO (compare these results with old DeltaDirectionsDistributions01)\n",
    "# for k, df in merg_resampled_dfs_dict.items():\n",
    "#     print(k, 'all', len(df))\n",
    "#     print(k,'1+3', len(df.loc[(df['near_wall']==1) | (df['near_wall']==3)]))\n",
    "#     print(k,'ratio', (len(df.loc[(df['near_wall']==1) | (df['near_wall']==3)]) / len(df)))\n",
    "# for k, df in dfs_dict_with_angles.items():\n",
    "#     subtraj_list = []\n",
    "#     for sub_df in df:\n",
    "#         sub_df = sort_walls_area(sub_df, bord40)\n",
    "#         #plot_wall_sorted_samples(sub_df, k)\n",
    "#         subtraj_list.append(sub_df)\n",
    "#     dfs_with_walls[k] = subtraj_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f180f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #delete TODO\n",
    "# #show to Katya Compare\n",
    "# #ок, будем считать что это +- похоже на старые резы\n",
    "# for k, df in merg_resampled_dfs_dict.items():\n",
    "#     plt.figure()\n",
    "#     plt.title(k)\n",
    "#     plt.plot(df.iloc[:50]['time'],df.iloc[:50]['angles'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7cf89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete TODO\n",
    "# make the same 13 and 24 from merged df\n",
    "# altDfsStep13, altDfsStep24 = {}, {}\n",
    "# for i, df in merg_resampled_dfs_dict.items():\n",
    "#     altDfsStep13[i] = df.loc[(df['near_wall']==1) | (df['near_wall']==3)]\n",
    "#     altDfsStep13[i] = altDfsStep13[i].reset_index()\n",
    "#     altDfsStep13[i] = altDfsStep13[i].drop(['index'], axis=1)\n",
    "#     altDfsStep24[i] = df.loc[(df['near_wall']==2) | (df['near_wall']==4)]\n",
    "#     altDfsStep24[i] = altDfsStep24[i].reset_index()\n",
    "#     altDfsStep24[i] = altDfsStep24[i].drop(['index'], axis=1)\n",
    "\n",
    "# delete TODO\n",
    "# for k, df in altDfsStep13.items():\n",
    "#     print(k, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete TODO\n",
    "# for k, df in altDfsStep13.items():\n",
    "#     print(k, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ff740",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge delete\n",
    "merg_resampled_dfs_dict = {}\n",
    "for k, df in dfs_with_walls.items():\n",
    "    #df = [df0.iloc[1:].reset_index() for df0 in df]\n",
    "    merg_resampled_dfs_dict[k] = pd.concat(df)\n",
    "    merg_resampled_dfs_dict[k] = merg_resampled_dfs_dict[k].reset_index(drop=True)\n",
    "for k,df in merg_resampled_dfs_dict.items():\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete\n",
    "DfsStep13, DfsStep24 = {}, {}\n",
    "n = 0\n",
    "for k, df in old_merg_resampled_dfs_dict.items():\n",
    "    DfsStep13[k] = df.loc[(df['near_wall']==1) | (df['near_wall']==3)]\n",
    "    DfsStep13[k] = DfsStep13[k].reset_index()\n",
    "    DfsStep13[k] = DfsStep13[k].drop(['index'], axis=1) \n",
    "    n += len(DfsStep13[k])\n",
    "\n",
    "    DfsStep24[k] = df.loc[(df['near_wall']==2) | (df['near_wall']==4)]\n",
    "    DfsStep24[k] = DfsStep24[k].reset_index()\n",
    "    DfsStep24[k] = DfsStep24[k].drop(['index'], axis=1)\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e935237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge delete\n",
    "old_merg_resampled_dfs_dict = {}\n",
    "for k, df in dfs_with_walls.items():\n",
    "    #df = [df0.iloc[1:].reset_index() for df0 in df]\n",
    "    old_merg_resampled_dfs_dict[k] = pd.concat(df)\n",
    "    old_merg_resampled_dfs_dict[k] = old_merg_resampled_dfs_dict[k].reset_index(drop=True)\n",
    "for k,df in old_merg_resampled_dfs_dict.items():\n",
    "    print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7453f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#old make DfsStep13, DfsStep24 from not merged dfs\n",
    "DfsStep13, DfsStep24 = {}, {}\n",
    "for k, df in dfs_with_walls.items():\n",
    "    list_k_13 = []\n",
    "    for sub_df in df:\n",
    "        sub_df = sub_df.loc[(sub_df['near_wall']==1) | (sub_df['near_wall']==3)]\n",
    "        list_k_13.append(sub_df)\n",
    "    DfsStep13[k] = pd.concat(list_k_13)\n",
    "    DfsStep13[k] = DfsStep13[k].reset_index()\n",
    "    DfsStep13[k] = DfsStep13[k].drop(['index'], axis=1) \n",
    "\n",
    "    list_k_24 = []\n",
    "    for sub_df in df:\n",
    "        sub_df = sub_df.loc[(sub_df['near_wall']==2) | (sub_df['near_wall']==4)]\n",
    "        list_k_24.append(sub_df)\n",
    "    DfsStep24[k] = pd.concat(list_k_24)\n",
    "    DfsStep24[k] = DfsStep24[k].reset_index()\n",
    "    DfsStep24[k] = DfsStep24[k].drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e5195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #delete TODO compare\n",
    "# #good\n",
    "# for k, df in DfsStep13.items():\n",
    "#     print(k, len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d7ca78",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=0\n",
    "for i, df in near_wall_dict.items():\n",
    "    for i, ddf in df.items():\n",
    "        q += len(ddf)\n",
    "print(q)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "navigation",
   "language": "python",
   "name": "navigation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
